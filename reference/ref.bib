@article{alqaryoutiServerlessComputingScheduling2018,
  title = {Serverless {{Computing}} and {{Scheduling Tasks}} on {{Cloud}}: {{A Review}}},
  author = {Alqaryouti, Omar and Siyam, Nur},
  year = {2018},
  volume = {40},
  number = {1},
  abstract = {Recently, the emergence of Function-as-a-Service (FaaS) has gained increasing attention by researchers. FaaS, also known as serverless computing, is a new concept in cloud computing that allows the services computation that triggers the code execution as a response for certain events. In this paper, we discuss various proposals related to scheduling tasks in clouds. These proposals are categorized according to their objective functions, namely minimizing execution time, minimizing execution cost, or multi objectives (time and cost). The dependency relationships between the tasks plays a vital role in determining the efficiency of the scheduling approach. This dependency may result in resources underutilization. FaaS is expected to have a significant impact on the process of scheduling tasks. This problem can be reduced by adopting a hybrid approach that combines both the benefit of FaaS and Infrastructure-as-a-Service (IaaS). Using FaaS, we can run the small tasks remotely and focus only on scheduling the large tasks. This helps in increasing the utilization of the resources because the small tasks will not be considered during the process of scheduling. An extension of the restricted time limit by cloud vendors will allow running the complete workflow using the serverless architecture, avoiding the scheduling problem.},
  langid = {english},
  keywords = {综述},
  file = {..\..\..\Zotero\storage\WXN537VH\Alqaryouti 和 Siyam - 2018 - Serverless Computing and Scheduling Tasks on Cloud.pdf}
}

@article{downeyParallelWorkloadModel1998,
  title = {A Parallel Workload Model and Its Implications for Processor Allocation},
  author = {Downey, Allen B.},
  year = {1998},
  month = may,
  journal = {Cluster Computing},
  pages = {133--145},
  doi = {10.1023/A:1019077214124},
  abstract = {We develop a workload model based on the observed behavior of parallel computers at the San Diego Supercomputer Center and the Cornell Theory Center. This model gives us insight into the performance of strategies for scheduling moldable jobs on space-sharing parallel computers. We find that Adaptive Static Partitioning (ASP), which has been reported to work well for other workloads, does not perform as well as strategies that adapt better to system load. The best of the strategies we consider is one that explicitly reduces allocations when load is high (a variation of Sevcik's (1989) A+ strategy).},
  langid = {english},
  file = {..\..\..\Zotero\storage\BPYPTDPM\Downey - A parallel workload model and its implications for.pdf}
}

@inproceedings{leslieExploitingPerformanceCost2013,
  title = {Exploiting {{Performance}} and {{Cost Diversity}} in the {{Cloud}}},
  booktitle = {2013 {{IEEE Sixth International Conference}} on {{Cloud Computing}}},
  author = {Leslie, Luke M. and {Young Choon Lee} and {Peng Lu} and Zomaya, Albert Y.},
  year = {2013},
  month = jun,
  pages = {107--114},
  publisher = {{IEEE}},
  address = {{Santa Clara, CA}},
  doi = {10.1109/CLOUD.2013.73},
  urldate = {2023-08-20},
  abstract = {Infrastructure-as-a-Service (IaaS) platforms, such as Amazon EC2, allow clients access to massive computational power in the form of virtual machines (VMs) known as instances. Amazon hosts three different instance purchasing options, each with its own service level agreement covering availability and pricing. In addition, Amazon offers access to a number of geographical regions, zones, and instance types from which to select. In this paper, we present a resource allocation and job scheduling framework (RAMC-DC), which utilizes Amazon's rich selection of service offerings\textemdash particularly within Spot and On-Demand instance purchasing options\textemdash aiming to cost efficiently execute deadline-constrained jobs. The framework is capable of ensuring quality of service in terms of cost, deadline compliance and service reliability. Such capacities are realized incorporating a set of novel strategies including execution time and cost approximation, bidding and resource allocation strategies. To the best of our knowledge, RAMC-DC most extensively exploits the service diversity of Amazon EC2, and offers a comprehensive cost efficiency solution that is able to deliver both the performance and reliability of On-Demand instances and the low costs of Spot instances. Experimental results obtained from extensive simulations using Amazon's Spot price traces show that our approach keeps deadline breaches and early-termination rates as low as 0.47\% and 0.18\%, respectively. This reliable performance is achieved with total costs between 13\% and 20\% of an equivalent approach using only On-Demand instances.},
  isbn = {978-0-7695-5028-2},
  langid = {english},
  keywords = {任务调度},
  file = {..\..\..\Zotero\storage\BLXF5NN8\Leslie 等 - 2013 - Exploiting Performance and Cost Diversity in the C.pdf}
}

@article{liServerlessComputingSurvey2022,
  title = {The {{Serverless Computing Survey}}: {{A Technical Primer}} for {{Design Architecture}}},
  shorttitle = {The {{Serverless Computing Survey}}},
  author = {Li, Zijun and Guo, Linsong and Cheng, Jiagan and Chen, Quan and He, Bingsheng and Guo, Minyi},
  year = {2022},
  month = jan,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {10s},
  eprint = {2112.12921},
  primaryclass = {cs},
  pages = {1--34},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3508360},
  urldate = {2023-08-20},
  abstract = {Note: This paper has been accepted by ACM Computing Surveys (CSUR), and the current e-print version is our major revision. For a complete view, please visit ACM CSUR. CCS Concepts: \textbullet{} Computer systems organization \textrightarrow{} Cloud computing; n-tier architectures; \textbullet{} Networks \textrightarrow{} Cloud computing; \textbullet{} Theory of computation \textrightarrow{} Parallel computing models.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {综述},
  file = {..\\..\\..\\Zotero\\storage\\LYYSNR3B\\The Serverless Computing Survey A Technical Primer for Design Architecture.xmind;..\\..\\..\\Zotero\\storage\\ZNZY8FY2\\Li 等 - 2022 - The Serverless Computing Survey A Technical Prime.pdf}
}

@misc{mampageDeepReinforcementLearning2023,
  title = {A {{Deep Reinforcement Learning}} Based {{Algorithm}} for {{Time}} and {{Cost Optimized Scaling}} of {{Serverless Applications}}},
  author = {Mampage, Anupama and Karunasekera, Shanika and Buyya, Rajkumar},
  year = {2023},
  month = aug,
  number = {arXiv:2308.11209},
  eprint = {2308.11209},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-08-31},
  abstract = {Serverless computing has gained a strong traction in the cloud computing community in recent years. Among the many benefits of this novel computing model, the rapid autoscaling capability of user applications takes prominence. However, the offer of adhoc scaling of user deployments at function level introduces many complications to serverless systems. The added delay and failures in function request executions caused by the time consumed for dynamically creating new resources to suit function workloads, known as the cold-start delay, is one such very prevalent shortcoming. Maintaining idle resource pools to alleviate this issue often results in wasted resources from the cloud provider perspective. Existing solutions to address this limitation mostly focus on predicting and understanding function load levels in order to proactively create required resources. Although these solutions improve function performance, the lack of understanding on the overall system characteristics in making these scaling decisions often leads to the sub-optimal usage of system resources. Further, the multi-tenant nature of serverless systems requires a scalable solution adaptable for multiple co-existing applications, a limitation seen in most current solutions. In this paper, we introduce a novel multi-agent Deep Reinforcement Learning based intelligent solution for both horizontal and vertical scaling of function resources, based on a comprehensive understanding on both function and system requirements. Our solution elevates function performance reducing cold starts, while also offering the flexibility for optimizing resource maintenance cost to the service providers. Experiments conducted considering varying workload scenarios show improvements of up to 23\% and 34\% in terms of application latency and request failures, while also saving up to 45\% in infrastructure cost for the service providers.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {弹性虚拟机集群},
  file = {..\..\..\Zotero\storage\CTEI7W7C\Mampage 等 - 2023 - A Deep Reinforcement Learning based Algorithm for .pdf}
}

@article{mnihAsynchronousMethodsDeep,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P and Silver, David and Kavukcuoglu, Koray},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  langid = {english},
  keywords = {未读},
  file = {..\..\..\Zotero\storage\MED3V4RV\Mnih 等 - Asynchronous Methods for Deep Reinforcement Learni.pdf}
}

@article{tangDiscretizingContinuousAction2020,
  title = {Discretizing {{Continuous Action Space}} for {{On-Policy Optimization}}},
  author = {Tang, Yunhao and Agrawal, Shipra},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {04},
  pages = {5981--5988},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i04.6059},
  urldate = {2023-09-22},
  abstract = {In this work, we show that discretizing action space for continuous control is a simple yet powerful technique for on-policy optimization. The explosion in the number of discrete actions can be efficiently addressed by a policy with factorized distribution across action dimensions. We show that the discrete policy achieves significant performance gains with state-of-theart on-policy optimization algorithms (PPO, TRPO, ACKTR) especially on high-dimensional tasks with complex dynamics. Additionally, we show that an ordinal parameterization of the discrete distribution can introduce the inductive bias that encodes the natural ordering between discrete actions. This ordinal architecture further significantly improves the performance of PPO/TRPO.},
  langid = {english},
  keywords = {未读},
  file = {..\..\..\Zotero\storage\JX5EU5YA\Tang 和 Agrawal - 2020 - Discretizing Continuous Action Space for On-Policy.pdf}
}

@inproceedings{wangDistributedMachineLearning2019,
  title = {Distributed {{Machine Learning}} with a {{Serverless Architecture}}},
  booktitle = {{{IEEE INFOCOM}} 2019 - {{IEEE Conference}} on {{Computer Communications}}},
  author = {Wang, Hao and Niu, Di and Li, Baochun},
  year = {2019},
  month = apr,
  pages = {1288--1296},
  publisher = {{IEEE}},
  address = {{Paris, France}},
  doi = {10.1109/INFOCOM.2019.8737391},
  urldate = {2023-08-20},
  abstract = {The need to scale up machine learning, in the presence of a rapid growth of data both in volume and in variety, has sparked broad interests to develop distributed machine learning systems, typically based on parameter servers. However, since these systems are based on a dedicated cluster of physical or virtual machines, they have posed non-trivial cluster management overhead to machine learning practitioners and data scientists. In addition, there exists an inherent mismatch between the dynamically varying resource demands during a model training job and the inflexible resource provisioning model of current cluster-based systems.},
  isbn = {978-1-72810-515-4},
  langid = {english},
  file = {..\..\..\Zotero\storage\PP834YDP\Wang 等 - 2019 - Distributed Machine Learning with a Serverless Arc.pdf}
}

@inproceedings{zhengFlowConElasticFlow2019,
  title = {{{FlowCon}}: {{Elastic Flow Configuration}} for {{Containerized Deep Learning Applications}}},
  shorttitle = {{{FlowCon}}},
  booktitle = {Proceedings of the 48th {{International Conference}} on {{Parallel Processing}}},
  author = {Zheng, Wenjia and Tynes, Michael and Gorelick, Henry and Mao, Ying and Cheng, Long and Hou, Yantian},
  year = {2019},
  month = aug,
  pages = {1--10},
  publisher = {{ACM}},
  address = {{Kyoto Japan}},
  doi = {10.1145/3337821.3337868},
  urldate = {2023-08-20},
  abstract = {An increasing number of companies are using data analytics to improve their products, services, and business processes. However, learning knowledge effectively from massive data sets always involves nontrivial computational resources. Most businesses thus choose to migrate their hardware needs to a remote cluster computing service (e.g., AWS) or to an in-house cluster facility which is often run at its resource capacity. In such scenarios, where jobs compete for available resources utilizing resources effectively to achieve high-performance data analytics becomes desirable. Although cluster resource management is a fruitful research area having made many advances (e.g., YARN, Kubernetes), few projects have investigated how further optimizations can be made specifically for training multiple machine learning (ML) / deep learning (DL) models. In this work, we introduce FlowCon, a system which is able to monitor loss functions of ML/DL jobs at runtime, and thus to make decisions on resource configuration elastically. We present a detailed design and implementation of FlowCon, and conduct intensive experiments over various DL models. Our experimental results show that FlowCon can strongly improve DL job completion time and resource utilization efficiency, compared to existing approaches. Specifically, FlowCon can reduce the completion time by up to 42.06\% for a specific job without sacrificing the overall makespan, in the presence of various DL job workloads.},
  isbn = {978-1-4503-6295-5},
  langid = {english},
  file = {..\..\..\Zotero\storage\I2FHLWF7\Zheng 等 - 2019 - FlowCon Elastic Flow Configuration for Containeri.pdf}
}
